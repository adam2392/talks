<!DOCTYPE html>
<html>
  <head>
    <title>L2F</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="fonts/quadon/quadon.css">
    <link rel="stylesheet" href="fonts/gentona/gentona.css">
    <link rel="stylesheet" href="slides_style.css">
    <script type="text/javascript" src="assets/plotly/plotly-latest.min.js"></script>
  </head>
  <body>
    <textarea id="source">


name:opening


**A Theory and Practice of the Lifelong Learnable**<br>
Hayden Helm | Ronak Mehta <br>
Carey E. Priebe | Raman Arora | [Joshua T. Vogelstein](https://neurodata.io) (JHU)  <br>


<!-- {[BME](https://www.bme.jhu.edu/),[ICM](https://icm.jhu.edu/),[CIS](http://cis.jhu.edu/),[KNDI](http://kavlijhu.org/)}@[JHU](https://www.jhu.edu/)  -->


<a href="https://neurodata.io"><img src="images/neurodata_purple.png" style="height:430px;"/></a>


<!-- <img src="images/funding/jhu_bme_blue.png" STYLE="HEIGHT:95px;"/> -->
<!-- <img src="images/funding/KNDI.png" STYLE="HEIGHT:95px;"/> -->
<!-- <font color="grey"></font> -->
.foot[[jovo&#0064;jhu.edu](mailto:jovo@jhu.edu) | <http://neurodata.io/talks> | [@neuro_data](https://twitter.com/neuro_data)]


---
## Outline

- Definitions: What is Lifelong Learning? 
- Algorithm: Lifelong Learning Forests (L2F)
- Numerical Results: L2F Lifelong Learns
- Discussion

---
class: middle, inverse

## .center[What is Lifelong Learning?]

---

## What is Learning?


<img src="images/Vapnik71.png" style="width:500px;"/>
--
<img src="images/Valiant84.png" style="width:500px;"/>

---

## What is Learning?

<img src="images/Mitchell97.png" style="width:600px;"/>


---


## What is  Learning? 

Tom Mitchell 1997 (not exact quote):


"An .r[algorithm] $f$  learns  in  .r[setting] $S$ with  data $D_n$ from distribution $P$ when  $f$'s .r[performance]  $\mathcal{E}$  improves with increasing sample size $n$."



---

### What is a Setting?

A setting is a sextuple $S = \lbrace \mathcal{Z}, \mathcal{A}, \mathcal{P}, \mathcal{H}, \ell, {R} \rbrace$

| Definition | Notation | Example
|:--- |:--- |:--- | 
| Measurements  | $ \mathcal{Z}$ |  $z_i = (x_i,y_i): i \in [n]$ |
|  Model | $\mathcal{P} := \lbrace P_\theta : \theta \in \Theta \rbrace$ | Gaussian
| Action space |  $\mathcal{A}$ |  turn left
| Hypotheses  | $\mathcal{H} = \lbrace h: \mathcal{Z} \to \mathcal{A} \rbrace$  | $a = \mathbb{I}(z>0)$
| Loss Function | $\ell: \Phi \times \mathcal{A} \to \mathbb{R}_+$  | $ (a - y)^2$
| Risk Functional | $R: \mathcal{L} \times \mathcal{P} \times \mathcal{H} \to \mathbb{R}_+$  | $\mathbb{E}_P[\ell( h(z), a)]$



---

### What is an Algorithm?

$f$ is a sequence of learning procedures, $f_1, f_2, \ldots$, where each $f_n \in \mathcal{F}$ maps from an element of data space to an action, $f : \mathcal{Z} \to \mathcal{A}$.


### What is  Performance?

Generalization error $\mathcal{E}$ is the expected risk with respect to all possibly training datasets weighted by their probability:

$$ \mathcal{E}\_S(f\_n,P) = \mathbb{E}_{P}[R(f_n(D_n))].$$



---
## What is  Learning?


"An .r[algorithm] $f$  learns  in  .r[setting] $S$ with  data $D_n$ from distribution $P$ when  $f$'s .r[performance]  $\mathcal{E}$  improves with increasing sample size $n$."



$$ \forall n \; \mathcal{E}\_S(f\_n,P) \geq \mathcal{E}\_S(f\_{n+1},P)$$ 
<!-- \; \\& \;  -->
$$\exists \; n \text{ s.t. } \mathcal{E}\_S(f\_n,P) > \mathcal{E}\_S(f\_{n+1},P) .$$ 


---

### What is Learning Efficiency?

The relative efficiency of $f\_n$ relative to $f'\_n$ is 
$$RE\_{S,P}(f\_n,f'\_n) := \frac{\mathcal{E}\_S(f\_n,P)}{\mathcal{E}\_S(f'\_n,P)}.$$

--

<br>

The goal of machine learning is to design new $f$'s that are increasingly more efficient. 

---

### What is Transfer Learning? 



An algorithm $f$ transfer learns  in setting $S$ with  data $D\_n$ from distribution $P$ when  $f$'s performance  $\mathcal{E}$  improves with increasing sample size $n$ faster when also including ${n'}$ samples from distribution $Q$


$$\frac{\mathcal{E}\_S(f\_{n+n'},P)}{\mathcal{E}\_S(f\_{n},P)} < 1 - \delta \qquad \qquad (1) $$

<br>

--

We say that $f$ .r[universally] transfers in setting $S$ if an only if $(1)$ is true for all distributions $P,Q \in \mathcal{P}$.


---

### What is Multi-Task Learning?



An algorithm $f$ multi-task learns  in settings $S\_1, \ldots, S\_J$
 with  data from distributions $P\_1, \ldots, P\_J$ when  $f$'s performance for each $(S\_j,P\_j)$ pair,  $\mathcal{E}\_j$, improves with increasing sample size $n\_j$ faster (on average) when also  including the other samples



$$ \frac{1}{J}\sum\_{j=1}^J \frac{\mathcal{E}\_j(f\_{\sum\_j n\_j},P\_j)}{\mathcal{E}\_j(f\_{n_j},P\_j)} < 1 - \delta \qquad \qquad (2) $$ 


---

### What is Lifelong Learning?




An algorithm $f$ lifelong learns  in a universe of settings $\mathcal{S}$
 with  data from distributions in $\mathcal{P}$ when  $f$'s performance $\mathcal{E}\_j$ for each sequentially observed $(S\_j,P\_j)$ pair improves with increasing sample size $n\_j$, on average faster when also  including both **future and past** data, and performance **never**  gets much worse.



$$\frac{1}{J}\sum\_{j=1}^J  \frac{\mathcal{E}\_j(f\_{\sum\_j n\_j},P\_j)}{\mathcal{E}\_j(f\_{n_j},P\_j)} < 1 - \delta \text{ (better on average)}$$

<br>

$$\frac{1}{J}\sum\_{j=1}^J  \frac{\mathcal{E}\_j(f\_{\sum\_j n\_j},P\_j)}{\mathcal{E}\_j(f\_{n_j},P\_j)} \leq 1 + \epsilon \text{ (never much worse)}$$

---
class: center, middle, inverse

(pause)


---
class: middle, inverse


## .center[Lifelong Learning Forests (L2F)]

---
### Algorithm (High-Level)
An algorithm has access to a collection of data from each distribution $ P\_1, ..., P\_J $. 
  
Suppose that each setting is a supervised $K$-class classification setting in $ \mathbb{R}^{d} $.
  
Our goal is to construct an interpretable and adaptable (to more tasks) classification procedure.
  
- Build forest for each task (each tree in forest induces a partition of $ \mathbb{R}^{d} $
- For task $j$, populate partitions of each forest (from every task) with held out task $j$ data. 
- Use these empiral data distributions to estimate posteriors P(Y = y | X = x)
- Resulting classifer: argmax (average posterior), where average is over all tasks                                                                                                          

  
---     
  
  
class: center, middle, inverse

(pause)


---
  
class: center, middle, inverse

## .center[L2F Lifelong Learns]

---

<iframe src="images/vip_results_parity_not_parity_n200.pdf" style="width:750px;"/>
                                                                                                                                                                                                                                                                                                                              
---    
  
<iframe src="images/vip_results_parity_rotated_parity_n200.pdf" style="width:750px;"/>
      
---                                                                                                            

## CIFAR 10-by-10

- cite other papers using it
- explain what it is 

---

<img src="images/initial-cifar-100-debugged-1-subsample-precision.png" style="width:750px;"/>

---

<img src="images/initial-cifar-100-debugged-1-subsample-recall.png" style="width:750px;"/>

---

## Next Steps 

- Prove the necessary and sufficient conditions under which L2F satisfy lifelong learning
- Extend empirical and theoretical results to deep learning 

---

### References 

1. C. Shen and J. T. Vogelstein. Decision Forests Induce Characteristic Kernels. arXiv, 2018
3. T. M. Tomita et al. Random Projection Forests. arXiv, 2018.
7. J. Browne et al. Forest Packing: Fast, Parallel Decision Forests. SIAM ICDM, 2018.
6. C. Shen et al. Discovering and Deciphering Relationships Across Disparate Data Modalities. eLife, 2019.
5. C. Shen, C. E. Priebe and J. T. Vogelstein. From Distance Correlation to Multiscale Graph Correlation. JASA, 2018.
8. Y. Lee, C. Shen and J. T. Vogelstein. Network Dependence Testing via Diffusion Maps and Distance-Based Correlations. Biometrika, 2018.
2. C. Shen and J. T. Vogelstein. The Exact Equivalence of Distance and Kernel Methods for Hypothesis Testing. arXiv, 2018.
1. M. Madhya, et al. Geodesic Learning via Unsupervised Decision Forests. arXiv, 2019.

Code: [https://neurodata.io/sporf/](https://neurodata.io/sporf/)




---
### Acknowledgements



<!-- <div class="small-container">
  <img src="faces/ebridge.jpg"/>
  <div class="centered">Eric Bridgeford</div>
</div>

<div class="small-container">
  <img src="faces/pedigo.jpg"/>
  <div class="centered">Ben Pedigo</div>
</div>

<div class="small-container">
  <img src="faces/jaewon.jpg"/>
  <div class="centered">Jaewon Chung</div>
</div> -->


<div class="small-container">
  <img src="faces/yummy.jpg"/>
  <div class="centered">yummy</div>
</div>

<div class="small-container">
  <img src="faces/lion.jpg"/>
  <div class="centered">lion</div>
</div>

<div class="small-container">
  <img src="faces/violet.jpg"/>
  <div class="centered">baby girl</div>
</div>

<div class="small-container">
  <img src="faces/family.jpg"/>
  <div class="centered">family</div>
</div>



<div class="small-container">
  <img src="faces/earth.jpg"/>
  <div class="centered">earth</div>
</div>


<div class="small-container">
  <img src="faces/milkyway.jpg"/>
  <div class="centered">milkyway</div>
</div>

<br>

--


<!-- <div class="small-container">
  <img src="faces/cep.png"/>
  <div class="centered">Carey Priebe</div>
</div> -->

<div class="small-container">
  <img src="faces/randal.jpg"/>
  <div class="centered">Randal Burns</div>
</div>


<div class="small-container">
  <img src="faces/cshen.jpg"/>
  <div class="centered">Cencheng Shen</div>
</div>


<div class="small-container">
  <img src="faces/bruce_rosen.jpg"/>
  <div class="centered">Bruce Rosen</div>
</div>


<div class="small-container">
  <img src="faces/kent.jpg"/>
  <div class="centered">Kent Kiehl</div>
</div>

<!-- <div class="small-container">
  <img src="faces/mim.jpg"/>
  <div class="centered">Michael Miller</div>
</div>

<div class="small-container">
  <img src="faces/dtward.jpg"/>
  <div class="centered">Daniel Tward</div>
</div> -->


<!-- <div class="small-container">
  <img src="faces/vikram.jpg"/>
  <div class="centered">Vikram Chandrashekhar</div>
</div>


<div class="small-container">
  <img src="faces/drishti.jpg"/>
  <div class="centered">Drishti Mannan</div>
</div> -->

<div class="small-container">
  <img src="faces/jesse.jpg"/>
  <div class="centered">Jesse Patsolic</div>
</div>

<div class="small-container">
  <img src="faces/falk_ben.jpg"/>
  <div class="centered">Benjamin Falk</div>
</div>

<!-- <div class="small-container">
  <img src="faces/kwame.jpg"/>
  <div class="centered">Kwame Kutten</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/perlman.jpg"/>
  <div class="centered">Eric Perlman</div>
</div> -->

<div class="small-container">
  <img src="faces/loftus.jpg"/>
  <div class="centered">Alex Loftus</div>
</div>

<!-- <div class="small-container">
  <img src="faces/bcaffo.jpg"/>
  <div class="centered">Brian Caffo</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/minh.jpg"/>
  <div class="centered">Minh Tang</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/avanti.jpg"/>
  <div class="centered">Avanti Athreya</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/vince.jpg"/>
  <div class="centered">Vince Lyzinski</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/dpmcsuss.jpg"/>
  <div class="centered">Daniel Sussman</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/youngser.jpg"/>
  <div class="centered">Youngser Park</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/shangsi.jpg"/>
  <div class="centered">Shangsi Wang</div>
</div> -->

<div class="small-container">
  <img src="faces/tyler.jpg"/>
  <div class="centered">Tyler Tomita</div>
</div>

<div class="small-container">
  <img src="faces/james.jpg"/>
  <div class="centered">James Brown</div>
</div>

<!-- <div class="small-container">
  <img src="faces/disa.jpg"/>
  <div class="centered">Disa Mhembere</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/gkiar.jpg"/>
  <div class="centered">Greg Kiar</div>
</div> -->


<div class="small-container">
  <img src="faces/jeremias.png"/>
  <div class="centered">Jeremias Sulam</div>
</div>

<!-- 
<div class="small-container">
  <img src="faces/meghana.png"/>
  <div class="centered">Meghana Madhya</div>
</div>
   -->

<!-- <div class="small-container">
  <img src="faces/percy.png"/>
  <div class="centered">Percy Li</div>
</div>
     -->

<!-- <div class="small-container">
  <img src="faces/hayden.png"/>
  <div class="centered">Hayden Helm</div>
</div> -->


<!-- <div class="small-container">
  <img src="faces/satish.png"/>
  <div class="centered">Satish Palaniappan</div>
</div> -->



<div class="small-container">
  <img src="faces/nate.png"/>
  <div class="centered">Nate Anderson</div>
</div>

<div class="small-container">
  <img src="faces/fan.png"/>
  <div class="centered">Qiuyun Fan</div>
</div>


 

</div>
<!-- <img src="images/funding/nsf_fpo.png" STYLE="HEIGHT:95px;"/> -->
<!-- <img src="images/funding/nih_fpo.png" STYLE="HEIGHT:95px;"/> -->
<img src="images/funding/darpa_fpo.png" STYLE=" HEIGHT:95px;"/>
<!-- <img src="images/funding/iarpa_fpo.jpg" STYLE="HEIGHT:95px;"/> -->
<!-- <img src="images/funding/KAVLI.jpg" STYLE="HEIGHT:95px;"/> -->
<!-- <img src="images/funding/schmidt.jpg" STYLE="HEIGHT:95px;"/> -->

---
class:center

<img src="images/liono_holds_babyv.jpeg" style="position:absolute; top:0px; left:200px; height:100%;"/>


</textarea>
<!-- <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script> -->
<script src="remark-latest.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
<script type="text/javascript">
  var options = {};
  var renderMath = function() {
    renderMathInElement(document.body);
    // or if you want to use $...$ for math,
    renderMathInElement(document.body, {delimiters: [ // mind the order of delimiters(!?)
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\[", right: "\\]", display: true},
        {left: "\\(", right: "\\)", display: false},
    ]});
  }
  var slideshow = remark.create(options, renderMath);

</script>
</body>
</html>
